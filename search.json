[
  {
    "objectID": "agenda.html",
    "href": "agenda.html",
    "title": "Agenda",
    "section": "",
    "text": "Programming basics recap\nMore Julia fundamentals\n\nThe type system & multiple dispatch\nPackages, environments and project organization\nPerformance optimization\n\nCode organization and git / github\n\n\n\n\nThe official Julia documentation\nThe Julia discourse forum\nYoutube video on developing Julia packages\nThe git and github documentation\nOnline git tutorials"
  },
  {
    "objectID": "agenda.html#data-science-toolkit",
    "href": "agenda.html#data-science-toolkit",
    "title": "Agenda",
    "section": "Data Science Toolkit",
    "text": "Data Science Toolkit\n\nTabular data and DataFrames.jl\nVisualization and Makie.jl\nNetwork analysis and Graphs.jl\n\n\nReferences & Materials\n\nThe docs: Makie.jl, DataFrames.jl, Graphs.jl\nThe Beautiful Makie website, with many nice plotting examples\nThe Julia Data Science online book, containing many simple examples\nThe 2022 book Julia for Data Analysis\nThe blog of Bogumli Kaminski, author of the DataFrames.jl package"
  },
  {
    "objectID": "agenda.html#bayesian-modeling",
    "href": "agenda.html#bayesian-modeling",
    "title": "Agenda",
    "section": "Bayesian Modeling",
    "text": "Bayesian Modeling\n\nProbabilistic programming with Turing.jl\nProbabilistic programming with Stan\nLow-level interface with LogDensityProblems.jl, TransformVariables.jl and DynamicHMC.jl\n\n\nReferences & Materials\n\nThe book Statistical Rethinking and Youtube lecture by Richard McElreath\nThe book Regression and Other Stories by Andrew Gelman et al.\nThe excellent Stan documentation"
  },
  {
    "objectID": "agenda.html#project-topics",
    "href": "agenda.html#project-topics",
    "title": "Agenda",
    "section": "Project Topics",
    "text": "Project Topics\n\nMain path analysis and MainPaths.jl\nPatent analytics\n\nJuliaPatents: PatentsBase.jl, PatentsLens.jl, PatentsLandscapes.jl\nData sources: EPO PATSTAT, Lens.org\n\nSurvey analysis"
  },
  {
    "objectID": "agenda.html#further-topics",
    "href": "agenda.html#further-topics",
    "title": "Agenda",
    "section": "Further Topics",
    "text": "Further Topics\n\nAgent-based models and Agents.jl\nDifferential equation models and DifferentialEquations.jl"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Julia",
    "section": "",
    "text": "This website contains materials used in a self-organized learning endeavour that includes programming (primarily in the Julia language), statistics (primarily of the Bayesian kind) and all other things that are of interest to us (e.g., network analysis and patent analytics)."
  },
  {
    "objectID": "pages/bayes/coinflip_ldp.html",
    "href": "pages/bayes/coinflip_ldp.html",
    "title": "Bayesian inference for a sequence of coinflips",
    "section": "",
    "text": "This is a tutorial on how to use the LogDensityProblems.jl ecosystem for Bayesian inference. Compared to other packages, such as Turing.jl, this approach is a bit more low-level, with the upside of being more hackable and being insightful for learning purposes."
  },
  {
    "objectID": "pages/bayes/coinflip_ldp.html#setup",
    "href": "pages/bayes/coinflip_ldp.html#setup",
    "title": "Bayesian inference for a sequence of coinflips",
    "section": "Setup",
    "text": "Setup\nFor this exercise, we’re interested in performing inference on a simple data-generating process where a possibly biased coin is flipped \\(N=100\\) times. More formally, we could state this like so:\n\\[\ny_i \\sim \\mathrm{Bernoulli}(p) \\textrm{ for } i = 1,2,...,100\n\\]\nWe start by simulating data from a Bernoulli distribution with the probability of heads set to \\(p = 0.7\\), which for the inverse problem is going to be the unknown quantity of interest to be inferred from observed data.\n\nusing Distributions\n\n\nN = 100\np = 0.7\nd = Bernoulli(p)\ndata = rand(d, N);\n\n\nNote: The avid reader will notice that this problem could also be more efficiently represented by making use of independence between the flips, in which case we could just record the total number of flips and the number of heads and make use of the binomial distribution. However, for sake of consistency we here stick to the Bernoulli representation."
  },
  {
    "objectID": "pages/bayes/coinflip_ldp.html#model-definition",
    "href": "pages/bayes/coinflip_ldp.html#model-definition",
    "title": "Bayesian inference for a sequence of coinflips",
    "section": "Model definition",
    "text": "Model definition\nHaving simulated data for inference, we now proceed to the model definition using the LogDensityProblems interface package. We store the flips in a struct called CoinflipProblem:\n\nstruct CoinflipProblem\n  flips::Vector{Bool}\nend;\n\nThe centerpiece of most modern Bayesian inference methods is the unnormalized log probability density function of the posterior distribution, which indicates how well a given parameter value fits the evidence provided by our data (encoded in the loglikelihood function) and our prior beliefs. For our numerical estimation procedure, it guides our search of the parameter space to obtain a representative set of draws from the posterior distribution.\nWe start by specifying the loglikelihood function, which, for a given value \\(p\\), is the sum of the log probability densities of each of our flips under a \\(\\textrm{Bernoulli}(p)\\) distribution:\n\nfunction loglik(p::Real, flips::Vector{Bool})\n  sum(y -> logpdf(Bernoulli(p), y), flips)\nend;\n\nNext to the likelihood function which makes use of the information from the data, we also need to specify a prior distribution which encodes our prior (before having seen the data) belief about \\(p\\). Here, we’re going to be broadly skeptical of extremely biased coins and use a \\(\\mathrm{Beta}(2,2)\\) prior:\n\nfunction logpri(p::Real)\n  logpdf(Beta(2,2), p)\nend;\n\nHere’s a plot of what that looks like:\n\nusing CairoMakie\nplot(Beta(2,2))\n\n\n\n\nWe now make our problem struct callable on an input parameter \\(\\theta\\), which is just a ‘container’ holding our actual parameter of interest, \\(p\\), but could also contain other parameters. Calling the struct on a given \\(\\theta\\) returns the sum of the log density of the prior and the log likelihood, a.k.a. the log posterior density evaluated at \\(p\\).\n\nfunction (problem::CoinflipProblem)(θ)\n  (; flips) = problem\n  (; p) = θ\n  logpri(p) + loglik(p, flips)\nend  \n\nWe can instantiate our CoinflipProblem with the data and call it to evaluate the log posterior density at a couple of values for \\(p\\):\n\nproblem = CoinflipProblem(data)\nproblem((; p=0.1)), problem((; p=0.5)), problem((; p=0.9))\n\n(-178.14130558275895, -68.90925294788643, -63.88562756127579)\n\n\nWhile the actual values of the log posterior density are not immediately that useful, we can already infer that, given the data and our prior beliefs, \\(p=0.5\\) is deemed similarly likely compared to \\(p=0.9\\) and much more likely than \\(p=0.1\\)."
  },
  {
    "objectID": "pages/bayes/coinflip_ldp.html#model-estimation",
    "href": "pages/bayes/coinflip_ldp.html#model-estimation",
    "title": "Bayesian inference for a sequence of coinflips",
    "section": "Model estimation",
    "text": "Model estimation\nHaving defined a way to evaluate the posterior density for a given parameter value, we now want a full representation of the posterior distribution to draw conclusions about the coin. While for simple problems, like the one presented here, a closed-form analytical solution is available, a more general method which also works for complicated models is to draw a large number of samples from the posterior distribution. Based on these samples, one can easily derive statements about certain summaries of the posterior distribution (e.g., its mean and standard deviation) or visualize it.\nA general purpose numerical procedure for obtaining samples from the posterior distribution, using the unnormalized log posterior density function (as specified above) and its gradient, is Hamiltonian Monte Carlo (HMC) and its variants.\nAs more of an implementation detail, HMC operates on the unconstrained reals but our parameter \\(p\\) is confined to the unit interval \\((0,1)\\) so we need an appropriate transformation, which is conveniently available in the TransformedLogDensities package. As mentioned, HMC furthermore requires the gradient of the posterior density, which we can conveniently obtain via automatic differentiation, in this case using the ForwardDiff package.\n\nusing LogDensityProblems\nusing TransformVariables, TransformedLogDensities\nusing LogDensityProblemsAD, ForwardDiff\n\ntransformation = as((p=as_unit_interval,))\ntran = TransformedLogDensity(transformation, problem)\ngrad = ADgradient(:ForwardDiff, tran)\n\nForwardDiff AD wrapper for TransformedLogDensity of dimension 1, w/ chunk size 1\n\n\nWe can now evaluate the logdensity and its gradient:\n\nLogDensityProblems.logdensity_and_gradient(grad, zeros(1))\n\n(-70.29554730900632, [26.0])\n\n\nWith this in place, we can now draw a large number of samples (say, \\(S=2000\\)) from the posterior distribution using the HMC implementation in DynamicHMC. We use the ThreadsX package to sample \\(k\\) chains in parallel:\n\nusing Random\nusing DynamicHMC\nusing ThreadsX\n\nfunction sample(grad, S, k; rng=Random.default_rng()) \n   ThreadsX.map(1:k) do _\n     mcmc_with_warmup(rng, grad, S; reporter=NoProgressReport())\n   end\nend\n\nresult = sample(grad, 2000, 4)\n\n4-element Vector{NamedTuple{(:posterior_matrix, :tree_statistics, :κ, :ϵ), Tuple{Matrix{Float64}, Vector{DynamicHMC.TreeStatisticsNUTS}, GaussianKineticEnergy{LinearAlgebra.Diagonal{Float64, Vector{Float64}}, LinearAlgebra.Diagonal{Float64, Vector{Float64}}}, Float64}}}:\n (posterior_matrix = [1.2243320240048454 1.3505728005340087 … 0.9648983867386188 0.9511446563142261], tree_statistics = [DynamicHMC.TreeStatisticsNUTS(-57.58857274844444, 2, turning at positions -3:0, 0.8468808154046951, 3, DynamicHMC.Directions(0x8ffc0d28)), DynamicHMC.TreeStatisticsNUTS(-57.28576889498291, 1, turning at positions -1:0, 0.8856491239379906, 1, DynamicHMC.Directions(0x6e976692)), DynamicHMC.TreeStatisticsNUTS(-57.36954909658937, 2, turning at positions -2:1, 0.9999999999999999, 3, DynamicHMC.Directions(0x4453d989)), DynamicHMC.TreeStatisticsNUTS(-57.22316434688225, 2, turning at positions -1:2, 0.9903498070846705, 3, DynamicHMC.Directions(0x839030de)), DynamicHMC.TreeStatisticsNUTS(-57.041505505125635, 1, turning at positions 1:2, 0.919205236116896, 3, DynamicHMC.Directions(0x7c1a75c6)), DynamicHMC.TreeStatisticsNUTS(-57.876086735047906, 2, turning at positions -3:0, 0.9059891382038067, 3, DynamicHMC.Directions(0xb1c237cc)), DynamicHMC.TreeStatisticsNUTS(-58.34195718721788, 1, turning at positions -1:0, 0.9462799234455904, 1, DynamicHMC.Directions(0x429a4cb4)), DynamicHMC.TreeStatisticsNUTS(-58.96472424199219, 1, turning at positions 0:1, 0.8571720646690528, 1, DynamicHMC.Directions(0xe49ceae9)), DynamicHMC.TreeStatisticsNUTS(-58.83729839719087, 2, turning at positions -1:2, 0.9999999999999999, 3, DynamicHMC.Directions(0x3a58ca4a)), DynamicHMC.TreeStatisticsNUTS(-60.4184272633255, 1, turning at positions 0:1, 0.5193190622108069, 1, DynamicHMC.Directions(0xe8d6a7a9))  …  DynamicHMC.TreeStatisticsNUTS(-57.54280334844213, 1, turning at positions -2:-3, 0.9985894428091329, 3, DynamicHMC.Directions(0x4d59bdf0)), DynamicHMC.TreeStatisticsNUTS(-58.02290675055895, 1, turning at positions 1:2, 0.6804669407768512, 3, DynamicHMC.Directions(0x730168d6)), DynamicHMC.TreeStatisticsNUTS(-57.39250303263262, 2, turning at positions -3:0, 0.8752155380776846, 3, DynamicHMC.Directions(0xcae9d0d4)), DynamicHMC.TreeStatisticsNUTS(-56.792314537073985, 1, turning at positions 0:1, 1.0, 1, DynamicHMC.Directions(0x17970b9f)), DynamicHMC.TreeStatisticsNUTS(-58.08349037420959, 2, turning at positions -3:0, 0.7538448855964478, 3, DynamicHMC.Directions(0x84004304)), DynamicHMC.TreeStatisticsNUTS(-57.84823376232417, 2, turning at positions -1:2, 0.8550484183982663, 3, DynamicHMC.Directions(0x60b06bd6)), DynamicHMC.TreeStatisticsNUTS(-57.031389061277885, 2, turning at positions 0:3, 0.9382832035259733, 3, DynamicHMC.Directions(0x2cb7373b)), DynamicHMC.TreeStatisticsNUTS(-56.810065845097235, 1, turning at positions 2:3, 0.9921454727462006, 3, DynamicHMC.Directions(0xa6f6bee7)), DynamicHMC.TreeStatisticsNUTS(-56.949876801161864, 1, turning at positions 1:2, 0.9543582734301151, 3, DynamicHMC.Directions(0xcd2a055a)), DynamicHMC.TreeStatisticsNUTS(-57.95389639659914, 2, turning at positions -3:0, 0.7849037017200189, 3, DynamicHMC.Directions(0x0c077d04))], κ = Gaussian kinetic energy (Diagonal), √diag(M⁻¹): [0.2173247725039289], ϵ = 1.1433307488023312)\n (posterior_matrix = [1.380263717936062 1.294744753355254 … 1.1713210698105851 1.1162070421511452], tree_statistics = [DynamicHMC.TreeStatisticsNUTS(-57.525571856033764, 1, turning at positions 0:1, 0.972242607304619, 1, DynamicHMC.Directions(0xa1dc904d)), DynamicHMC.TreeStatisticsNUTS(-57.36480773717284, 2, turning at positions -1:2, 0.9999999999999999, 3, DynamicHMC.Directions(0xc7e8b2ae)), DynamicHMC.TreeStatisticsNUTS(-57.06016984564197, 1, turning at positions -1:0, 1.0, 1, DynamicHMC.Directions(0x43bd8778)), DynamicHMC.TreeStatisticsNUTS(-57.71433368162087, 2, turning at positions 0:3, 0.9230761183109188, 3, DynamicHMC.Directions(0x07d035b7)), DynamicHMC.TreeStatisticsNUTS(-56.89686315143345, 2, turning at positions -2:1, 0.9999999999999999, 3, DynamicHMC.Directions(0x0ed75541)), DynamicHMC.TreeStatisticsNUTS(-57.13381700803753, 1, turning at positions 1:2, 0.8982954722819919, 3, DynamicHMC.Directions(0xe6e131d2)), DynamicHMC.TreeStatisticsNUTS(-56.76460204145688, 2, turning at positions -3:0, 0.9883672331759596, 3, DynamicHMC.Directions(0xd878b760)), DynamicHMC.TreeStatisticsNUTS(-57.086530966547706, 2, turning at positions -3:0, 0.9508368591206664, 3, DynamicHMC.Directions(0x8ecd134c)), DynamicHMC.TreeStatisticsNUTS(-57.93074017548754, 2, turning at positions -4:-5, 0.861790445755022, 7, DynamicHMC.Directions(0xec68bdba)), DynamicHMC.TreeStatisticsNUTS(-57.02535135532046, 1, turning at positions 2:3, 0.9974760082889899, 3, DynamicHMC.Directions(0x6e2cf5bb))  …  DynamicHMC.TreeStatisticsNUTS(-57.0885270879587, 2, turning at positions -3:0, 0.9553241493984661, 3, DynamicHMC.Directions(0xb2b1b278)), DynamicHMC.TreeStatisticsNUTS(-57.15937810447227, 1, turning at positions 1:2, 0.9624102705674793, 3, DynamicHMC.Directions(0x25204a42)), DynamicHMC.TreeStatisticsNUTS(-57.001085245942754, 1, turning at positions 0:1, 0.9618186897270251, 1, DynamicHMC.Directions(0x3ec77897)), DynamicHMC.TreeStatisticsNUTS(-56.93122912941795, 2, turning at positions -2:1, 0.9999999999999999, 3, DynamicHMC.Directions(0xaaa0003d)), DynamicHMC.TreeStatisticsNUTS(-57.07241963032904, 1, turning at positions 1:2, 0.9403853089184561, 3, DynamicHMC.Directions(0x26db4eea)), DynamicHMC.TreeStatisticsNUTS(-57.330891070495476, 1, turning at positions -1:0, 0.9484472234323174, 1, DynamicHMC.Directions(0xf693bffa)), DynamicHMC.TreeStatisticsNUTS(-57.26118011195611, 1, turning at positions -2:-3, 0.9964663479113812, 3, DynamicHMC.Directions(0x8790a294)), DynamicHMC.TreeStatisticsNUTS(-59.95690535995078, 2, turning at positions -3:0, 0.5999608965891455, 3, DynamicHMC.Directions(0xb1ef70e0)), DynamicHMC.TreeStatisticsNUTS(-56.74207211324079, 1, turning at positions -1:0, 0.991844421494833, 1, DynamicHMC.Directions(0x1f74fcf0)), DynamicHMC.TreeStatisticsNUTS(-57.87071389380655, 2, turning at positions 0:3, 0.8442401790852164, 3, DynamicHMC.Directions(0xcde8cdbf))], κ = Gaussian kinetic energy (Diagonal), √diag(M⁻¹): [0.2119609122869729], ϵ = 1.049327377055609)\n (posterior_matrix = [1.5012456300283699 1.6252515821694793 … 1.1584952489775988 0.878751539536218], tree_statistics = [DynamicHMC.TreeStatisticsNUTS(-59.92942839763377, 1, turning at positions 0:1, 1.0, 1, DynamicHMC.Directions(0x047eb115)), DynamicHMC.TreeStatisticsNUTS(-59.234006852102404, 1, turning at positions 0:1, 0.8383254882327325, 1, DynamicHMC.Directions(0x2f92ecc7)), DynamicHMC.TreeStatisticsNUTS(-61.88036129511939, 1, turning at positions -1:0, 0.6486411014443525, 1, DynamicHMC.Directions(0xc07add7c)), DynamicHMC.TreeStatisticsNUTS(-60.81236586919107, 2, turning at positions -3:-4, 0.9487045216068832, 7, DynamicHMC.Directions(0x1574de1b)), DynamicHMC.TreeStatisticsNUTS(-57.141231910713095, 1, turning at positions 0:1, 1.0, 1, DynamicHMC.Directions(0xa1728057)), DynamicHMC.TreeStatisticsNUTS(-56.8955193519844, 1, turning at positions 0:1, 1.0, 1, DynamicHMC.Directions(0xfccc6def)), DynamicHMC.TreeStatisticsNUTS(-56.992777686768704, 1, turning at positions 2:3, 0.9764499389750926, 3, DynamicHMC.Directions(0x6f6da977)), DynamicHMC.TreeStatisticsNUTS(-56.75847381091904, 1, turning at positions 0:1, 0.9993253358581085, 1, DynamicHMC.Directions(0xe80f4bd1)), DynamicHMC.TreeStatisticsNUTS(-58.607433167493674, 1, turning at positions 1:2, 0.7244414412935685, 3, DynamicHMC.Directions(0x735f7dca)), DynamicHMC.TreeStatisticsNUTS(-58.07740376518286, 1, turning at positions -2:-3, 0.9816815158467148, 3, DynamicHMC.Directions(0x3bab9004))  …  DynamicHMC.TreeStatisticsNUTS(-57.874650337054355, 1, turning at positions -2:-3, 0.9850456468046175, 3, DynamicHMC.Directions(0x2499e52c)), DynamicHMC.TreeStatisticsNUTS(-57.30751139575347, 1, turning at positions -1:-2, 0.9192326218569442, 3, DynamicHMC.Directions(0x84335ae1)), DynamicHMC.TreeStatisticsNUTS(-56.93123104548634, 2, turning at positions -1:2, 0.9999999999999999, 3, DynamicHMC.Directions(0x161c638a)), DynamicHMC.TreeStatisticsNUTS(-56.79773982025494, 1, turning at positions -1:0, 0.9871483358859661, 1, DynamicHMC.Directions(0xccd1a460)), DynamicHMC.TreeStatisticsNUTS(-57.20730735719754, 1, turning at positions 1:2, 0.9479592282859222, 3, DynamicHMC.Directions(0x38aa972e)), DynamicHMC.TreeStatisticsNUTS(-58.60610528016222, 1, turning at positions -1:-2, 0.7539651378364279, 3, DynamicHMC.Directions(0x12bbc34d)), DynamicHMC.TreeStatisticsNUTS(-57.45312706792343, 1, turning at positions -2:-3, 0.9907637393360593, 3, DynamicHMC.Directions(0x17f98544)), DynamicHMC.TreeStatisticsNUTS(-56.813401705690914, 2, turning at positions -1:2, 0.9999999999999999, 3, DynamicHMC.Directions(0xd9edb1aa)), DynamicHMC.TreeStatisticsNUTS(-56.76914879634394, 1, turning at positions 0:1, 1.0, 1, DynamicHMC.Directions(0x3d433c1b)), DynamicHMC.TreeStatisticsNUTS(-57.492432750310705, 1, turning at positions 1:2, 0.8768759186005548, 3, DynamicHMC.Directions(0x797250aa))], κ = Gaussian kinetic energy (Diagonal), √diag(M⁻¹): [0.23887177224061476], ϵ = 0.9123975802179018)\n (posterior_matrix = [1.344064529465467 1.021890724030394 … 1.2177376780620355 0.6475406379042593], tree_statistics = [DynamicHMC.TreeStatisticsNUTS(-57.922376575136774, 2, turning at positions -2:1, 0.9616434859799138, 3, DynamicHMC.Directions(0xa9114639)), DynamicHMC.TreeStatisticsNUTS(-57.819340598829555, 2, turning at positions 4:5, 0.939124228130856, 7, DynamicHMC.Directions(0x5a9fed75)), DynamicHMC.TreeStatisticsNUTS(-57.583489125389804, 2, turning at positions 0:3, 0.8986842520689496, 3, DynamicHMC.Directions(0x8b8bf0f3)), DynamicHMC.TreeStatisticsNUTS(-56.79019358969951, 2, turning at positions -3:0, 0.9898331940727747, 3, DynamicHMC.Directions(0xd4d1c158)), DynamicHMC.TreeStatisticsNUTS(-56.77546971709273, 1, turning at positions -2:-3, 0.9986103850950125, 3, DynamicHMC.Directions(0x2508915c)), DynamicHMC.TreeStatisticsNUTS(-57.27678466631426, 2, turning at positions 0:3, 0.9276877805569055, 3, DynamicHMC.Directions(0xfdb94efb)), DynamicHMC.TreeStatisticsNUTS(-57.09723830565446, 1, turning at positions -2:-3, 0.990841690647739, 3, DynamicHMC.Directions(0x15134200)), DynamicHMC.TreeStatisticsNUTS(-57.241763259903585, 1, turning at positions -1:-2, 0.9215805064334389, 3, DynamicHMC.Directions(0x677f3a45)), DynamicHMC.TreeStatisticsNUTS(-57.18787735558431, 1, turning at positions -2:-3, 0.9882182425027057, 3, DynamicHMC.Directions(0x149c193c)), DynamicHMC.TreeStatisticsNUTS(-56.69985330513813, 2, turning at positions 0:3, 0.9991773267678695, 3, DynamicHMC.Directions(0x69ddcc53))  …  DynamicHMC.TreeStatisticsNUTS(-57.38014428708953, 1, turning at positions -2:-3, 0.9767464099319563, 3, DynamicHMC.Directions(0x9e002198)), DynamicHMC.TreeStatisticsNUTS(-56.805122412196944, 2, turning at positions 0:3, 0.9851891321214451, 3, DynamicHMC.Directions(0xb2dd23d7)), DynamicHMC.TreeStatisticsNUTS(-56.84670465152834, 1, turning at positions 2:3, 0.9871812306199822, 3, DynamicHMC.Directions(0xaf8176b3)), DynamicHMC.TreeStatisticsNUTS(-57.04987959404856, 1, turning at positions -1:-2, 0.9548845648598396, 3, DynamicHMC.Directions(0xbe1ba3a5)), DynamicHMC.TreeStatisticsNUTS(-57.64252578626763, 2, turning at positions -4:-5, 0.9023818301216678, 7, DynamicHMC.Directions(0xdc519c02)), DynamicHMC.TreeStatisticsNUTS(-57.6254615102857, 2, turning at positions 0:3, 0.9438574215257529, 3, DynamicHMC.Directions(0xe684e657)), DynamicHMC.TreeStatisticsNUTS(-57.05946650438044, 1, turning at positions -1:0, 1.0, 1, DynamicHMC.Directions(0x1e07f7ec)), DynamicHMC.TreeStatisticsNUTS(-56.942630338254986, 1, turning at positions -1:0, 0.9983785316647114, 1, DynamicHMC.Directions(0x829b2a6a)), DynamicHMC.TreeStatisticsNUTS(-57.435322223032145, 2, turning at positions 5:6, 0.9281914234049401, 7, DynamicHMC.Directions(0x2cb9615e)), DynamicHMC.TreeStatisticsNUTS(-60.60009024627679, 2, turning at positions -3:0, 0.7239166983809384, 3, DynamicHMC.Directions(0x335aa07c))], κ = Gaussian kinetic energy (Diagonal), √diag(M⁻¹): [0.2075995125331578], ϵ = 0.9609152907151286)\n\n\nThe result is a vector of length \\(k\\), each element of which contains for each chain the posterior samples as well as some statistics about the sampling procedure, which can be used to check if everything went as planned."
  },
  {
    "objectID": "pages/bayes/coinflip_ldp.html#model-checking",
    "href": "pages/bayes/coinflip_ldp.html#model-checking",
    "title": "Bayesian inference for a sequence of coinflips",
    "section": "Model checking",
    "text": "Model checking\nHaving obtained samples from the posterior distribution, we’re in principle ready to use our model for inference, i.e., answer the question of whether our coin is biased and by how much, and how certain we can be of the answer based on the data we have seen.\nHowever, before we jump to inference, it is good practice to perform some model checks: Our estimates rely on a numerical sampling scheme, which can fail, rendering the results unreliable.\n\nusing MCMCDiagnosticTools\nusing DynamicHMC.Diagnostics\n\nFirst, we can check the effective sample size (ess). In Markov chain monte carlo (MCMC) approaches, samples are often correlated, meaning that the total number of ‘effective’ samples is less than obtained by an uncorrelated sampling procedure because consecutive samples carry some of the same information.\n\ness, Rhat =  ess_rhat(stack_posterior_matrices(result))\n\n([3193.591232118471], [1.000271694703177])\n\n\n\nsummarize_tree_statistics.(getfield.(result, :tree_statistics))\n\n4-element Vector{DynamicHMC.Diagnostics.TreeStatisticsSummary{Float64, NamedTuple{(:max_depth, :divergence, :turning), Tuple{Int64, Int64, Int64}}}}:\n Hamiltonian Monte Carlo sample of length 2000\n  acceptance rate mean: 0.91, 5/25/50/75/95%: 0.59 0.87 0.97 1.0 1.0\n  termination: divergence => 0%, max_depth => 0%, turning => 100%\n  depth: 0 => 0%, 1 => 61%, 2 => 39%\n Hamiltonian Monte Carlo sample of length 2000\n  acceptance rate mean: 0.93, 5/25/50/75/95%: 0.72 0.9 0.97 1.0 1.0\n  termination: divergence => 0%, max_depth => 0%, turning => 100%\n  depth: 0 => 0%, 1 => 63%, 2 => 37%\n Hamiltonian Monte Carlo sample of length 2000\n  acceptance rate mean: 0.93, 5/25/50/75/95%: 0.7 0.9 0.97 1.0 1.0\n  termination: divergence => 0%, max_depth => 0%, turning => 100%\n  depth: 0 => 0%, 1 => 63%, 2 => 37%\n Hamiltonian Monte Carlo sample of length 2000\n  acceptance rate mean: 0.94, 5/25/50/75/95%: 0.76 0.91 0.97 1.0 1.0\n  termination: divergence => 0%, max_depth => 0%, turning => 100%\n  depth: 0 => 0%, 1 => 57%, 2 => 43%"
  },
  {
    "objectID": "pages/bayes/coinflip_ldp.html#model-inference",
    "href": "pages/bayes/coinflip_ldp.html#model-inference",
    "title": "Bayesian inference for a sequence of coinflips",
    "section": "Model inference",
    "text": "Model inference\n\nusing StructArrays\n\nfunction posterior(result)\n  samples = eachcol(pool_posterior_matrices(result))\n  StructArray(transform.(transformation, samples))\nend\n\npost = posterior(result);\n\n\nfunction summarize(post)\n  m, s = round.((mean(post.p), std(post.p)); digits=2)\n  println(\"posterior mean: \", m)\n  println(\"posterior sd: \", s)\nend\n\nsummarize(post)\n\nposterior mean: 0.75\nposterior sd: 0.04\n\n\n\nfunction plot_inferred_vs_true(post, p_true)\n  fig = Figure(); ax = Axis(fig[1,1])\n  density!(ax, post.p; color=:grey20)\n  vlines!(ax, p_true; linewidth=2)\n  fig\nend\n\nplot_inferred_vs_true(post, p)"
  },
  {
    "objectID": "pages/programming_recap.html",
    "href": "pages/programming_recap.html",
    "title": "Programming Basics",
    "section": "",
    "text": "Exercise 1\nWrite a function that takes in a name and prints out a greeting, e.g., “Hello, Daniel”.\n\n\nSolution\ngreet(name) = println(\"Hello, $(name)!\")\ngreet(\"Daniel\")\n\n\nHello, Daniel!\n\n\n\n\nExercise 2\nWrite a function which greets the users whose name starts with a ‘D’ in Spanish, users whose name starts with a ‘C’ in German, and everyone else in English.\n\n\nSolution\nfunction greet(name)\n    firstletter = first(name)\n    if firstletter == 'D'\n        println(\"Hola, $(name)!\")\n    elseif firstletter == 'C'\n        println(\"Hallo, $(name)!\")\n    else\n        println(\"Hello, $(name)!\")\n    end\nend\n\ngreet(\"Denise\")\ngreet(\"Clara\")\ngreet(\"Marius\")\n\n\nHola, Denise!\nHallo, Clara!\nHello, Marius!\n\n\n\n\nExercise 3\nWrite a function which takes an array of numbers as input and returns their sum, without using the built-in function sum.\n\n\nSolution\nfunction mysum(arr)\n    res = zero(eltype(arr))\n    for x in arr\n        res += x\n    end\n    res\nend\n\n@show mysum([1,2,3,4,5]);\n\n\nmysum([1, 2, 3, 4, 5]) = 15\n\n\n\n\nExercise 4\nWrite a function which takes an array of numbers as input and returns their sum of squares.\n\n\nSolution\nsum_of_squares(arr) = sum(x -> x^2, arr) \n# or mapreduce(x -> x^2, +, arr)\n@show sum_of_squares([1,2,3]);\n\n\nsum_of_squares([1, 2, 3]) = 14\n\n\n\n\nExercise 5\nWrite a function which takes an array of numbers as input and returns the largest element.\n\n\nSolution\nlargest_element(arr) = findmax(arr)[1]\n@show largest_element([5,2,1,7]);\n\n\nlargest_element([5, 2, 1, 7]) = 7\n\n\n\n\nExercise 6\nWrite a function which takes an array of numbers as input and returns only those elements which are \\(>5\\). In a second step, write a more generic version which takes the limit as a second argument.\n\n\nSolution\ngreater_5(arr) = filter(>(5), arr)\ngreater_k(arr, k) = filter(>(k), arr)\n\n@show greater_5([1,2,3,4,5,6,7,8]);\n@show greater_k([1,2,3,4,5,6,7,8], 2);\n\n\ngreater_5([1, 2, 3, 4, 5, 6, 7, 8]) = [6, 7, 8]\ngreater_k([1, 2, 3, 4, 5, 6, 7, 8], 2) = [3, 4, 5, 6, 7, 8]\n\n\n\n\nExercise 7\nWrite a function which checks if an element is contained in an array.\n\n\nSolution\nx_in_arr(x, arr) = x in arr\n\n@show x_in_arr(\"Daniel\", [\"Denise\", \"Daniel\", \"Jakob\"]);\n\n\nx_in_arr(\"Daniel\", [\"Denise\", \"Daniel\", \"Jakob\"]) = true\n\n\n\n\nExercise 8\nWrite a function which takes a Matrix as input and returns the column-wise sums. In a second step, write a more generic version which takes an arbitrary reduction function (such as sum) as an additional argument and performs it column-wise.\n\n\nSolution\ncolsum(m) = sum(m; dims=1)\ncolop(op, m) = map(op, eachcol(m))\n\n@show colsum([1 2; 3 4]);\n@show colop(sum, [1 2; 3 4]);\n\n\ncolsum([1 2; 3 4]) = [4 6]\n\n\n\ncolop(sum, [1 2; 3 4]) = [4, 6]\n\n\n\n\nExercise 9\nWrite a function that concatenates two arrays. In a second step, write a function which concatenates two \\(n\\)-element arrays into a \\(n \\times 2\\) matrix.\n\n\nSolution\nconcatenate(a, b) = vcat(a, b)\nconcatenate_matrix(a, b) = hcat(a, b)\n\n@show concatenate([1,2], [3,4,5]);\n@show concatenate_matrix([1,2,3], [4,5,6]);\n\n\nconcatenate([1, 2], [3, 4, 5]) = [1, 2, 3, 4, 5]\n\n\nconcatenate_matrix([1, 2, 3], [4, 5, 6]) = [1 4; 2 5; 3 6]\n\n\n\n\nExercise 10\nWrite a function that takes a number and returns a function which multiplies its input by that number. Apply the generated function to each element of an array of 5 randomly generated numbers.\n\n\nSolution\ngenerate_mul_by_k(k) = x -> x * k\n\nmul_by_3 = generate_mul_by_k(3)\nmap(mul_by_3, rand(5))\n\n\n5-element Vector{Float64}:\n 0.25644331288873135\n 2.0530745950636446\n 1.1656951963331545\n 2.0972120196429693\n 2.4091204829156814"
  },
  {
    "objectID": "pages/statistical-rethinking/week1_globe_tossing.html",
    "href": "pages/statistical-rethinking/week1_globe_tossing.html",
    "title": "Statistical Rethinking",
    "section": "",
    "text": "About this Document\nThis writeup loosely follows Richard McElreath’s Statistical Rethinking YouTube lecture and book of the same name. As a programming environment it uses Julia (instead of R, as used in the lecture). Julia has a variety of packages on probabilistic programming and Bayesian inference, such as Turing.jl, LogDensityProblems.jl, or Stan.jl. This writeup focuses more on the technical implementation and assumes that the reader has followed the lecture for background and reasoning.\n\n\nBayesian Inference and Bayes Rule\nBayesian inference departs from a model for the joint distribution of all observed variables (data, \\(\\mathcal{D}\\)) and unobserved variables (parameters, \\(\\theta\\)):\n\\[ p(\\theta, \\mathcal{D}) = p(\\mathcal{D}|\\theta)p(\\theta) \\]\nThe factorization on the right hand side is a direct consequence of the definition of conditional probability (see box below).\nBayesian inference then relies on Bayes’ rule (from which it derives its name) to derive the posterior distribution of parameters \\(\\theta\\) conditional on observed data \\(\\mathcal{D}\\):\n\\[\np(\\theta | \\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta)p(\\theta)}{p({\\mathcal{D}})}\n\\]\n\nBayes Rule is just a restatement of the general definition of conditional probability:\n\\[\\mathrm{Pr}(A|B) := \\frac{\\mathrm{Pr}(A,B)}{\\mathrm{Pr}(B)}\\]\nThe numerator here is the joint probability, i.e. the probability that both events \\(A\\) and \\(B\\) occur. The joint probability is also sometimes written as \\(p(A \\cap B)\\) (an intersection in set notation) and is symmetric with respect to its arguments: \\[\\mathrm{Pr}(A,B) = \\mathrm{Pr}(A|B)\\mathrm{Pr}(B) = \\mathrm{Pr}(B|A)\\mathrm{Pr}(A)\\] If we now replace the joint probability by the its conditioned representation in the definition above, we arive at Bayes Rule: \\[\\mathrm{Pr}(A|B) = \\frac{\\mathrm{Pr}(B|A)\\mathrm{Pr}(A)}{\\mathrm{Pr}(B)}\\]\n\nIn the context of Bayes’ rule, \\(p(\\mathcal{D}|\\theta)\\) (taken as a function of \\(\\theta\\) and not \\(\\mathcal{D}\\)) is called the likelihood and specifies how the data influence our inferences. \\(p(\\theta)\\) is called the prior and formalizes our beliefs on the parameters before having seen the data (which might derive from earlier studies or common sense).\n\n\nGlobe Tossing\n\nSimulating the Data Generating Process\nWe start by writing a simple program which can simulate the globe tossing experiment. In this case, this just involves sampling ‘water’ or ‘land’ \\(N\\) times, with probabilities \\(\\theta\\) and \\(1-\\theta\\), respectively:\n\nusing StatsBase\n\nfunction simulate_globe(θ, N) \n    outcomes = [\"W\", \"L\"]\n    probabilities = [θ, 1-θ]\n    sample(outcomes, Weights(probabilities), N)\nend\n\nsimulate_globe(0.7, 5)\n\n5-element Vector{String}:\n \"W\"\n \"L\"\n \"W\"\n \"W\"\n \"W\"\n\n\nBecause in the end we just care about the count of ‘water’ throws among the \\(N\\) trials and the sum of independent Bernoulli trials is well modeled by the binomial distribution, we can also just use that to simulate our experiment:\n\nusing Distributions\n\nsimulate_globe_binomial(θ, N) = rand(Binomial(N, θ))\nsimulate_globe_binomial(0.7, 5)\n\n5\n\n\n\n\nThe Beta-Binomial Model\nFollowing the general process of Bayesian inference outlined above, we need a representation of the joint distribution of known and unknown variables, which we can factor into two components: (1) a sampling distribution that describes how the known variables are generated for given values of the unknowns and (2) a prior distribution over the unknowns.\nSampling distribution. The data generating process for the globe tossing example is well represented by a binomial sampling model, which specifies a distribution for the number of ‘successes’ (called \\(y\\), say occurrences of water) among a number of \\(N\\) trials, each with the same success probability \\(\\theta\\). To indicate that the random variable \\(y\\) follows a binomial distribution, we write:\n\\[ y \\sim \\mathrm{Binomial}(N, \\theta)\\]\nComputing the probability of an observed number of successes \\(y\\) for a given \\(\\theta\\) is then given by the probability mass function of the binomial distribution:\n\\[ p(y|\\theta) = \\mathrm{Binomial}(y | N, \\theta) = {N \\choose y} \\theta^y (1-\\theta)^{N-y},\\]\nwhere \\({N \\choose y} = \\frac{N!}{y!(N-y)!}\\) is called the binomial coefficient. In the context of the posterior distribution, where the right hand side of the above serves as the likelihood and is taken as a function of \\(\\theta\\), the binomial coefficient is constant (because it does not depend on \\(\\theta\\)), which is helpful in analytical derivations.\nHere is a plot of the binomial distribution for different values of \\(\\theta\\), with \\(N=50\\):\n\nusing CairoMakie\n\nfunction plotbinom(params)\n    tostring(p) = \"Binomial(50, $(p.θ))\"\n    fig = Figure()\n    ax = Axis(fig[1,1])\n    foreach(params) do p\n        d = Binomial(50, p.θ)\n        barplot!(ax, d; cycle=:color, label=tostring(p))\n    end\n    axislegend(ax; position=(:left, :top))\n    fig\nend\n\nparams = [(;θ=0.1), (;θ=0.5), (;θ=0.9)]\nplotbinom(params)\n\n\n\n\nPrior distribution. A natural prior for the parameter \\(\\theta\\) is the beta distribution, which is defined for the interval \\([0,1]\\) and thus fits the bounds of \\(\\theta\\) (which represents a proportion). The distribution has two paramteres, commonly called \\(\\alpha\\) and \\(\\beta\\), which control its shape.\nThe probability density function for the beta is given by:\n\\[\np(\\theta) = \\textrm{Beta}(\\theta | \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha - 1} (1-\\theta)^{\\beta - 1}\n\\]\nThe factor involving the gamma functions \\(\\Gamma(\\cdot)\\) is a normalizing constant ensuring that the pdf integrates to 1 over the sample space. Here is a plot of the pdf with different values for \\(\\alpha\\) and \\(\\beta\\):\n\nfunction plotbeta(params)\n    tostring(p) = \"Beta($(p.α), $(p.β))\"\n    fig = Figure()\n    ax = Axis(fig[1,1])\n    foreach(params) do p\n        d = Beta(p...)\n        plot!(ax, d; cycle=:color, label=tostring(p))\n    end\n    axislegend(ax; position=(:left, :top))\n    fig\nend\n\nparams = [(;α=1, β=1), (;α=2, β=2), (;α=3, β=2)]\nplotbeta(params)\n\n\n\n\nPosterior distribution. The posterior distribtution is proportional to the prior times the likelihood (which is just the factored joint distribution):\n\\[\np(\\theta|\\mathcal{D}) \\propto p(\\mathcal{D}|\\theta)p(\\theta)\n\\]\nFor many analytical and numerical procedures the normalizing constant \\(p(\\mathcal{D}\\)) can be dropped. We here also make use of this fact to analytically derive the closed form posterior distribution. This is not generally possible, which is why numerical applications are so important in Bayesian inference.\nFollowing the choices above, the unnormalized posterior distribution is then the product of a binomial likelihood and a beta prior:\n\\[\n\\begin{align}\np(\\theta|\\mathcal{D}) &\\propto \\textrm{Binomial}(y|N, \\theta) \\times \\textrm{Beta}(\\theta|\\alpha, \\beta) \\\\\n&= {N \\choose y} \\theta^y (1-\\theta)^{N-y} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha - 1} (1-\\theta)^{\\beta - 1}\n\\end{align}\n\\]\nWhile this looks intimidating at first, it is just the definitions for the binomial and beta pdf (or pmf) multiplied together. Just as we started by dropping the normalizing constant for the posterior distribution, we can now again drop the constants from the beta and binomial probability functions (i.e., everything not involving \\(\\theta\\), such as the binomial coefficient and the gamma functions):\n\\[\n\\begin{align}\np(\\theta|\\mathcal{D}) &\\propto \\theta^y (1-\\theta)^{N-y} \\theta^{\\alpha - 1} (1-\\theta)^{\\beta - 1} \\\\\n&= \\theta^{y + \\alpha - 1}(1-\\theta)^{N-y+\\beta-1}\n\\end{align}\n\\]\nThe result is the kernel of a beta distribution (i.e. a beta without the normalizing constant), from which we can conclude that the posterior is just another beta distribtution where the \\(\\alpha\\) and \\(\\beta\\) hyperparameters from the original prior distribution are updated based on the data, i.e.:\n\\[ \\theta | y \\sim \\textrm{Beta}(\\alpha + y, \\beta + N - y)\\]\nWrapped into a Julia function, this is a single line of code:\n\nglobe_posterior(y, N; α=2, β=2) = Beta(α + y, β + N - y);\n\nWith this, we’re now ready to run a simple simulation with, say, \\(N=100\\) throws:\n\n\nPlotting function\nfunction plot_prior_posterior(prior, posterior, true_θ)\n    fig = Figure(); ax = Axis(fig[1,1])\n    plot!(ax, prior; label=\"Prior\", cycle=:color)\n    plot!(ax, posterior; label=\"Posterior\", cycle=:color)\n    vlines!(ax, true_θ; label=\"True θ\", linewidth=3, color=:black)\n    axislegend(ax; position=(:left, :top))\n    fig\nend;\n\n\n\ntrue_θ = 0.7; N = 100\nthrows = simulate_globe(true_θ, N)\ny = sum(==(\"W\"), throws)\n\nprior = Beta(2,2)\nposterior = globe_posterior(y, N)\n\nplot_prior_posterior(prior, posterior, true_θ)\n\n\n\n\nBased on this, we can see how our diffuse prior on the proportion of water has been updated based on the data to a posterior that sits close to the true value of \\(\\theta = 0.7\\).\n\n\nDigression: Grid Approximation\nA different way to obtain the posterior distribution, relying on numerical approximation, is to compute the joint distribution (likelihood \\(\\times\\) prior) at a fine grid of values and then normalize this by dividing by the sum of the computed values.\n\njoint(N, y, θ) = pdf(Beta(1,1), θ) * pdf(Binomial(N, θ), y)\n\nfunction posterior_approx(N, y; P=1000)\n    grid = range(0, 1, P)\n    points = [joint(N, y, θ) for θ in grid]\n    posterior = points ./ sum(points)\n    sample(grid, Weights(posterior), 10_000; replace=true)\nend\n\npost = posterior_approx(N, y)\nhist(post; color=:grey80, strokewidth=1) \nvlines!(true_θ; color=:black, linewidth=2)\nxlims!(0, 1); current_figure()\n\n\n\n\nWhile this approach does not rely on mathematical conveniences such as conjugacy, it suffers from the curse of dimensionality: The number of grid points grows exponentially with the number of parameters and so grid approximation is only suited for very simiple problems.\n\n\n\nPrior & Posterior Predictions\nOften times, we are interested in the range of outcomes that would be reasonable to expect for a given prior or posterior distribution, i.e., the prior or posterior predictive distribution. Seeing how the model behaves for a given prior or posterior distribution is useful for model & research design, for model checking and validation, or for making forecasts.\nA particularly practical way to obtain the prior or posterior predictive distribution is via sampling many times from the prior or posterior and then for each sampled parameter value simulate an outcome from the observation model.\nFor the globe tossing example this could look like the following julia function:\n\nfunction predictive(N, prior_or_posterior; S=1000)\n    simulate(θ, N) = rand(Binomial(N, θ))\n    samples = rand(prior_or_posterior, S)\n    [simulate(θ, N) for θ in samples]\nend;\n\nWe can now use this to check what kinds of results the model would deem reasonable in an experiment with 100 globe throws, before seeing any data and just based on a flat \\(\\textrm{Beta}(1,1)\\) prior:\n\npriorpred = predictive(100, Beta(1,1))\nhist(priorpred; color=:grey80, strokewidth=1)\n\n\n\n\nThis indicates that, under the flat prior, a result of 0 out of 100 times water is seen as similarly likely as, e.g., 50 out of 100 times water, which is probably not that sensible.\nWe can perform a similar excercise but now based on the posterior distribution:\n\npostpred = predictive(100, posterior)\nhist(postpred; color=:grey80, strokewidth=1)\nxlims!(0, 100); current_figure()\n\n\n\n\nConsistent with the posterior distribution centered at 0.7, we can see that the model now expects around 70 out of 100 throws to result in ‘water’ and both values lower than 50 or close to 100 are deemed very unlikely."
  },
  {
    "objectID": "pages/statistical-rethinking/week2_categorical_data.html",
    "href": "pages/statistical-rethinking/week2_categorical_data.html",
    "title": "Statistical Rethinking",
    "section": "",
    "text": "Linear Regression with Categorical Inputs\nAfter having considered the case of weight as a function of height in the last lecture, we now move to building a model for categorical predictors. For the considered example of modeling weights, we start by investigating the total differences between men and women before building a multivariate model which stratifies the relationship between height and weight by sex.\n\n\nSimulate the Data Generating Process\nAs before, our first step is to build a simulation model of the data generating process.\n\nusing Distributions\nusing CairoMakie\nusing DataFrames\n\n\nfunction simulate_data(S, b, a)\n    H = ifelse(S == 1, 150, 160) + rand(Normal(0, 5))\n    W = a[S] + b[S]*H + rand(Normal(0, 5))\n    (;S, H, W)\nend;\n\nThis function accepts an index variable for sex (1 for women, 2 for men) and containers with coefficients and intercepts for men and women, respectively. It then returns a NamedTuple with the input sex, the simulated height and the simulated weight.\n\nβ = (0.5, 0.6)\nα = (0, 0)\nsimulate_data(1, β, α)\n\n(S = 1, H = 150.30737211935994, W = 75.36035572219524)\n\n\nUsing an array comprehension, we can simulate a population of randomly drawn sexes or populations for men and women seperatley:\n\ndat   = [simulate_data(S, β, α) for S in rand([1,2], 100)] |> DataFrame\nmen   = [simulate_data(2, β, α) for _ in 1:100] |> DataFrame\nwomen = [simulate_data(1, β, α) for _ in 1:100] |> DataFrame;\n\n\nfirst(dat, 4)\n\n\n4×3 DataFrameRowSHWInt64Float64Float6411153.1268.962121151.97872.876232163.06394.676641155.55973.8757\n\n\n \nGiven 100 pairs of simulated men and women, we can compute the weight contrast in our population by taking the mean of the pairwise differences:\n\nmean(men.W .- women.W)\n\n22.07552980949418\n\n\nWith this ground truth in place, we can now move towards creating an estimator which we can validate using the simulations.\n\n\nSpecify Model\n\n\nQuadratic approximation\nusing Turing\nusing Optim\nusing StatsBase: vcov\nusing LinearAlgebra\nusing StructArrays\n\nfunction quadratic_approximation(model)\n    est = optimize(model, MAP())\n    cov = Symmetric(vcov(est).array)\n    MvNormal(est.values.array, cov)\nend\n\nfunction posterior_samples(fit, names; S=10000)\n    r = rand(fit, S)\n    r = map(eachcol(r)) do s\n        NamedTuple{names}(s)\n    end\n    StructArray(r)\nend;\n\n\nFor the case of a direct comparison between women and men, a simple model with only two intercepts, one for women and one for men, does the job:\n\\[\n\\begin{align}\n\\textrm{weight}_i &\\sim \\textrm{Normal}(\\alpha_i, \\sigma) \\\\\n\\alpha_{S[i]} &\\sim \\textrm{Normal}(60, 10) \\text{ for } \\\\\n\\sigma &\\sim \\textrm{Exponential(10)}\n\\end{align}\n\\]\nHere, \\(S[i] = 1\\) if observation \\(i\\) is a woman and \\(S[i] = 2\\) if observation \\(i\\) is a man. The corresponding Turing.jl model makes use of filldist(dist, k) in the prior specification for \\(\\alpha\\), which creates a k-length array of distributions where each element follows distribution dist. The array \\(\\alpha\\) can then be indexed with α[sexes[i]] in the likelihood computation to pick the corresponding entry based on observation \\(i\\)’s sex.\n\n@model function model_compare_sexes(weights, sexes)\n    α ~ filldist(Normal(60, 10), 2)\n    σ ~ Exponential(10)\n    for i in eachindex(weights)\n        weights[i] ~ Normal(α[sexes[i]], σ)\n    end\nend;\n\nHaving defined the model structure, we can instantiate the model on the simulated data and fit it with the quadratic_approximation() function defined in the last session:\n\nmodel = model_compare_sexes(dat.W, dat.S)\nfit = quadratic_approximation(model)\n\nFullNormal(\ndim: 3\nμ: [75.7206148014402, 95.461182709039, 5.964015757589394]\nΣ: [0.579890939263621 0.0005804711320471676 -0.005400901000373145; 0.0005804711320471676 0.9058364174547548 -0.01899315886122377; -0.005400901000373145 -0.01899315886122377 0.17671881516666726]\n)\n\n\nAs before, the result is a multivariate normal distribution approximating the posterior distribution of the parameters which can be directly examined or sampled for posterior inference. In this case, the first two entries of this distribution’s parameter \\(\\mu\\) (the posterior mode) represent \\(\\alpha_1\\) and \\(\\alpha_2\\). The difference of about 20 between these is in line with the contrast computed earlier, so the model seems to adequately capture this part of the structure of our population.\n\n\nAnalyse Real Data\nHaving tested the model on simulated data, we can now move on to the real observations. We start again by downloading the data into a DataFrame and filter out the adults:\n\n\nDownload function\nusing HTTP, CSV\n\nfunction download_data(dataset)\n    repo = \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/\"\n    link = repo * dataset\n    file = HTTP.download(link; update_period=Inf)\n    CSV.read(file, DataFrame)\nend;\n\n\n\ndf = download_data(\"Howell1.csv\");\ndf = subset(df, :age => ByRow(>=(18)));\n\n┌ Warning: Reading one byte at a time from HTTP.Stream is inefficient.\n│ Use: io = BufferedInputStream(http::HTTP.Stream) instead.\n│ See: https://github.com/BioJulia/BufferedStreams.jl\n└ @ HTTP.Streams C:\\Users\\Jakob\\.julia\\packages\\HTTP\\z8l0i\\src\\Streams.jl:240\n\n\n\nfirst(df, 4)\n\n\n4×4 DataFrameRowheightweightagemaleFloat64Float64Float64Int641151.76547.825663.012139.736.485863.003136.52531.864865.004156.84553.041941.01\n\n\n \nWe then instantiate and fit the model, as for the simulated data, and draw some samples from the posterior distribution for computing posterior quantities of interest:\n\nmodel = model_compare_sexes(df.weight, df.male .+ 1)\nfit = quadratic_approximation(model)\nsamples = posterior_samples(fit, (:α1, :α2, :σ));\n\nOne set of such quantities of interest is the groupwise mean weights for men and women and the corresponding difference in means. For each of these, we can plot the posterior distribution computed from the posterior samples generated above:\n\n\nPlotting function\nfunction plot_posterior_means(samples)\n    fig = Figure(resolution = (900, 400))\n    ax1 = Axis(fig[1,1]; \n               title = \"Posterior distribution of groupwise means\",\n               titlealign=:left)\n\n    density!(ax1, samples.α1; color=(:red, 0.0), strokecolor=:red, strokewidth=2, label=\"women (α₁)\")    \n    density!(ax1, samples.α2; color=(:blue, 0.0), strokecolor=:blue, strokewidth=2, label=\"men (α₂)\")  \n    axislegend(ax1; nbanks=2, position=:lt)\n\n    diff = [s.α2 - s.α1 for s in samples]\n    \n    ax2 = Axis(fig[1,2]; title = \"Posterior distribution of difference in means\", titlealign=:left)\n    density!(ax2, diff; color=(:red, 0.0), strokecolor=:black, strokewidth=2)\n\n    fig\nend;\n\n\n\nplot_posterior_means(samples)\n\n\n\n\nWhile the difference in means indicates that on average, men are about 7 kilos heavier than women, this does not take the residual standard deviation \\(\\sigma\\) into account; despite the average difference, there can be women heavier than men. A more complete representation of the modeled population is then to simulate men and women from a Normal distribution, using the posterior draws for the \\(\\alpha\\)’s and \\(\\sigma\\):\n\n\nPlotting function\nfunction plot_posterior_prediction(samples)\n    fig = Figure(resolution=(900, 400))\n    ax1 = Axis(fig[1,1]; title = \"Groupwise posterior predictive distribution\", titlealign=:left)\n\n    women = [rand(Normal(s.α1, s.σ)) for s in samples]\n    men = [rand(Normal(s.α2, s.σ)) for s in samples]\n    diff = men .- women\n\n    density!(ax1, men; color=(:blue, 0.0), strokecolor=:blue, strokewidth=2, label=\"men (α₁)\")    \n    density!(ax1, women; color=(:red, 0.0), strokecolor=:red, strokewidth=2, label=\"women (α₂)\")   \n    axislegend(ax1; position=:lt) \n\n    ax2 = Axis(fig[1,2]; title = \"Posterior contrast / difference in prediction\", titlealign=:left)\n\n    k = Makie.KernelDensity.kde(diff)\n    i = findfirst(>(0), k.x)\n    p = round(sum(first(k.density, i) / sum(k.density)) * 100; digits=1)\n\n    lines!(ax2, k; color=:black)\n    band!(k.x[1:i], zeros(i), k.density[1:i]; color=:red)\n    band!(k.x[i+1:end], zeros(length(k.x) - i), k.density[i+1:end]; color=:blue)\n    \n    vlines!(ax2, 0; color=:black, linestyle=:dash)\n    text!(ax2, -20, 0.03; text=\"$p% \\nwomen\", color=:red)\n    text!(ax2,  20, 0.03; text=\"$(100-p)% \\nmen\", color=:blue)\n\n    fig\nend;\n\n\n\nplot_posterior_prediction(samples)\n\n\n\n\nThe posterior contrast shows that, if we were to sample pairs of men and women from the population, we would expect that in about 80% of the comparisons, the man would be heavier than the woman."
  },
  {
    "objectID": "pages/statistical-rethinking/week2_linear_regression.html",
    "href": "pages/statistical-rethinking/week2_linear_regression.html",
    "title": "Statistical Rethinking",
    "section": "",
    "text": "Get and Inspect Data\nIn the week 2 lecture, we learn techniques of linear regression in the investigation of the relationships between height, weight, age and sex. Initially, we only consider adults and focus weights as a function of height, before moving on to modelling nonlinearities and categorical inputs.\nThe first step is to download the data used in the lecture from its GitHub repository directly into a DataFrame.\n\nusing HTTP\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing CairoMakie\nusing Distributions\n\n\nfunction download_data(dataset)\n    repo = \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/\"\n    link = repo * dataset\n    file = HTTP.download(link)\n    CSV.read(file, DataFrame)\nend;\n\n\ndf = download_data(\"Howell1.csv\");\n\n┌ Warning: Reading one byte at a time from HTTP.Stream is inefficient.\n│ Use: io = BufferedInputStream(http::HTTP.Stream) instead.\n│ See: https://github.com/BioJulia/BufferedStreams.jl\n└ @ HTTP.Streams C:\\Users\\Jakob\\.julia\\packages\\HTTP\\z8l0i\\src\\Streams.jl:240\n\n\n┌ Info: Downloading\n│   source = \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv\"\n│   dest = \"C:\\\\Users\\\\Jakob\\\\AppData\\\\Local\\\\Temp\\\\Howell1.csv\"\n│   progress = NaN\n│   time_taken = \"0.96 s\"\n│   time_remaining = \"NaN s\"\n│   average_speed = \"12.468 KiB/s\"\n│   downloaded = \"11.919 KiB\"\n│   remaining = \"∞ B\"\n└   total = \"∞ B\"\n\n\nWe create a second dataset containing only adults, as the growth dynamics of children result in a quite different relationship between weight and height:\n\ndf18 = @rsubset(df, :age > 18);\n\nHere’s a plot of the full dataset and the adults-only dataset sidy-by-side:\n\n\nCode\nplotargs = (color=(:black, 0.0), strokewidth=4, strokecolor=(:crimson, .5))\n\nfig = Figure(resolution=(800, 400))\nax1 = Axis(fig[1,1]; title=\"Full data\", ylabel=\"weight (kg)\", xlabel=\"height (cm)\")\nax2 = Axis(fig[1,2]; title=\"Age > 18 years\", xlabel=\"height (cm)\")\n\nscatter!(ax1, df.height, df.weight; plotargs...)\nscatter!(ax2, df18.height, df18.weight; plotargs...)\n\nfig\n\n\n\n\n\n\n\nModeling the Data Generating Process\nFor the adults-only data, a simple linear regression model seems to capture the relationship between height and weight well. There are two equivalent ways to specify such a model. The first uses the direct specification of a distribution for our outcome observations:\n\\[\n\\begin{align}\n\\textrm{weight}_i &\\sim \\textrm{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\textrm{height}_i\n\\end{align}\n\\]\nThe second way instead places the distribution on the errors or residuals (\\(\\epsilon_i\\)), i.e., the deviations of the observations from the regression line:\n\\[\n\\begin{align}\n\\textrm{weight}_i &= \\alpha + \\beta \\textrm{height}_i + \\epsilon_i \\\\\n\\epsilon_i &\\sim \\textrm{Normal}(0, \\sigma)\n\\end{align}\n\\]\nAs for the globe tossing model in week 1, we specify a simulation procedure for this model to generate fake data for some height and known values of the parameters. We chose the second of the two above representations by generating a mean zero residual and adding it to the regression line at some height \\(H\\):\n\nfunction simulate_weight(H; β, σ)\n    U = rand(Normal(0, σ))\n    β*H + U\nend;\n\nNote that a full specification usually also includes the regression line intercept (often denoted \\(\\alpha\\)), which is implicitly set to zero in this simulation, i.e., a person with height zero is assumed to have weight zero.\nWith this function, we can, e.g., simulate the weight of a person that is 1,58m tall. If you run the function multiple times it will return different values. This is the case due to the DGP’s inherent randomness, controlled here by \\(\\sigma\\):\n\nsimulate_weight(158; β=0.5, σ=5)\n\n75.77067120822689\n\n\n\nheights = rand(Uniform(130, 170), 200)\nweights = simulate_weight.(heights; β=0.5, σ=5)\nscatter(heights, weights; figure=(;resolution=(600, 400)), label = \"Simulated data\", plotargs...)\nlines!([130, 170], [130, 170] .* 0.5; label = \"True regression line\")\naxislegend(position=:lt); current_figure()\n\n\n\n\n\n\nBayesian Inference\nWe are now ready to estimate the unknown parameters specifying the relationship between height and weight based on observed data. In the context of Bayesian inerence, this means that we need to specify the joint distribution, or equivalently the likelihood and prior. In Julia, one way to do this is to use the probabilistic programming language Turing.jl:\n\nusing Turing\nusing Optim\nusing StatsBase: vcov\nusing LinearAlgebra\nusing StructArrays\n\n\n@model function linear_regression(weights, heights)\n    # Prior\n    α ~ Normal(0, 10)\n    β ~ Normal(0, 1)\n    σ ~ Exponential(3)\n    # Likelihood\n    for i in eachindex(weights)\n        weights[i] ~ Normal(α + β*heights[i], σ)\n    end\nend;\n\nFor models of intermediate complexity, the lecture uses the quadratic or laplace approximation, which approximates the posterior distribution with a multivariate normal centered at its mode. The mode, i.e., the set of parameters for which the posterior density is maximized, can be found with some numerical optimization procedure:\n\nfunction quadratic_approximation(model)\n    est = optimize(model, MAP())\n    cov = Symmetric(vcov(est).array)\n    MvNormal(est.values.array, cov)\nend;\n\nWith this in place, we can now instantiate the model and produce a fit with the quadratic approximation:\n\nmodel = linear_regression(df18.weight, df18.height);\n\n\nfit = quadratic_approximation(model)\n\nFullNormal(\ndim: 3\nμ: [-42.659113058358365, 0.5672822698603196, 4.257909978099955]\nΣ: [17.56203140648791 -0.11327838941463268 0.09241706385513915; -0.11327838941463268 0.0007328523328169009 -0.000596122522172525; 0.09241706385513915 -0.000596122522172525 0.026525238921016457]\n)\n\n\nBecause the result is a distribution (a multivariate normal, as mentioned above), we can use the usual interface to, e.g., draw random samples from the posterior via rand(). We here wrap this into a small helper function which adds parameter names to the resulting random samples and wraps them into a StructArray for easier processing:\n\nfunction posterior_samples(fit; S=1000)\n    r = rand(fit, S)\n    r = map(eachcol(r)) do s\n        NamedTuple{(:α, :β, :σ)}(s)\n    end\n    StructArray(r)\nend;\n\nWith this function, it is now easy to draw a number regression lines from the posterior distribution and plot them with our data:\n\nfunction plot_model_and_data(heights, weights, fit)\n    fig = Figure(resolution=(800, 400))\n    ax = Axis(fig[1,1])\n    scatter!(ax, heights, weights; plotargs...)\n    samples = posterior_samples(fit; S=50)\n    for s in samples\n        lines!(ax, [130, 190], s.α .+ [130, 190] .* s.β; color=(:black, .4))\n    end\n\n    fig\nend; \n\n\nplot_model_and_data(df18.height, df18.weight, fit)\n\n\n\n\n\n\nDigression: Building an unnormalized posterior density by hand\nFor inference, Turing turns the model specification into a function evaluating the log joint for a given set of parameter values, which can then be used by some inference algorithm (such as the quadratic approximation). Instead of using the Turing specification, we could also build this function by hand.\nWe start with the (log) likelihood, which for a given set of parameter values is just the sum of the log probability densities of each observation’s weight under a normal distribution, with the mean given by the regression line induced by parameters \\(\\alpha\\) and \\(\\beta\\) at that observation’s height and residual standard deviation \\(\\sigma\\):\n\nfunction loglikelihood(α, β, σ; weights, heights)\n    sum(eachindex(weights)) do i\n        logpdf(Normal(α + β*heights[i], σ), weights[i])\n    end\nend;\n\nWe pick a set of parameter values with which to evaluate our function:\n\nα, β, σ = 0, 0.5, 5\n\n(0, 0.5, 5)\n\n\n\nloglikelihood(α, β, σ; weights, heights)\n\n-600.9075266361563\n\n\nSimilarly, the prior evaluates the log density for each parameter value under the corresponding prior distribution:\n\nfunction logprior(α, β, σ)\n    logpdf(Normal(0,10), α) + \n    logpdf(Normal(0,1), β) + \n    logpdf(Exponential(3), σ)\nend;\n\n\nlogprior(α, β, σ)\n\n-7.030741114738167\n\n\nThe log joint is then just the sum of the log likelihood and the log prior:\n\nfunction logjoint(α, β, σ; weights, heights) \n    loglikelihood(α, β, σ; weights, heights) + logprior(α, β, σ)\nend;\n\n\nlogjoint(α, β, σ; weights, heights)\n\n-607.9382677508944"
  }
]